{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daisuke134/MW_detection/blob/main/MW_DETECTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16BCdoOTlX38"
      },
      "outputs": [],
      "source": [
        "# 複数のチャンクサイズで試す+スレッド並列処理\n",
        "\n",
        "import mne\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def load_data(fname, chunk_size):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "    total_time = 0\n",
        "    for start in range(0, raw.n_times, chunk_size):\n",
        "        start_time = time.time()\n",
        "        data_chunk = raw.get_data(start=start, stop=start+chunk_size)\n",
        "        total_time += time.time() - start_time\n",
        "    return total_time\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]  # ファイルリスト\n",
        "\n",
        "# 複数のチャンクサイズで試す\n",
        "chunk_sizes = [50000, 100000, 150000, 200000, 250000, 300000, 500000, 1000000]\n",
        "best_time = float('inf')\n",
        "best_chunk_size = None\n",
        "\n",
        "for chunk_size in chunk_sizes:\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        times = list(executor.map(lambda fname: load_data(fname, chunk_size), bdf_files))\n",
        "    avg_time = sum(times) / len(times)\n",
        "    if avg_time < best_time:\n",
        "        best_time = avg_time\n",
        "        best_chunk_size = chunk_size\n",
        "\n",
        "print(f\"最適なチャンクサイズ: {best_chunk_size}, 平均読み込み時間: {best_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HytOrlkv9N7d"
      },
      "outputs": [],
      "source": [
        "#スレッドによる並列化＋タスクの分割\n",
        "\n",
        "import mne\n",
        "import time\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def load_data_chunk(fname, start, stop):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "    data_chunk = raw.get_data(start=start, stop=stop)\n",
        "    return data_chunk\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "\n",
        "chunk_size = 1000000  # チャンクサイズの設定\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    future_to_chunk = {executor.submit(load_data_chunk, fname, start, start+chunk_size): fname for fname in bdf_files for start in range(0, raw.n_times, chunk_size)}\n",
        "    for future in concurrent.futures.as_completed(future_to_chunk):\n",
        "        fname = future_to_chunk[future]\n",
        "        try:\n",
        "            data_chunk = future.result()\n",
        "        except Exception as exc:\n",
        "            print(f'{fname} generated an exception: {exc}')\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"ファイル読み込みにかかった時間: {elapsed_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HroMihn082ds"
      },
      "outputs": [],
      "source": [
        "#マルチプロセス\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "import mne\n",
        "import time\n",
        "\n",
        "def process_file(fname, n_times):\n",
        "    def load_data_chunk(start, stop):\n",
        "        raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "        data_chunk = raw.get_data(start=start, stop=stop)\n",
        "        return data_chunk\n",
        "\n",
        "    chunk_size = 1000000\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        start_time = time.time()\n",
        "        chunks = [executor.submit(load_data_chunk, start, start+chunk_size) for start in range(0, n_times, chunk_size)]\n",
        "        results = [future.result() for future in concurrent.futures.as_completed(chunks)]\n",
        "        total_time = time.time() - start_time\n",
        "    return total_time\n",
        "\n",
        "def execute_process_file(args):\n",
        "    return process_file(*args)\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "#(fname)とn_timesのリストを取得\n",
        "file_n_times = [(fname, mne.io.read_raw_bdf(fname, preload=False).n_times) for fname in bdf_files]\n",
        "\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    times = list(executor.map(execute_process_file, file_n_times))\n",
        "\n",
        "avg_time = sum(times) / len(times)\n",
        "print(f\"平均読み込み時間: {avg_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwZwjm1uZino"
      },
      "outputs": [],
      "source": [
        "#ヒートマップ作成\n",
        "# 必要なライブラリをインストール\n",
        "!pip install mne matplotlib\n",
        "\n",
        "# ライブラリをインポート\n",
        "import mne\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ファイル名リストの例 (適宜修正してください)\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "\n",
        "# ヒートマップを作成する関数\n",
        "def create_heatmap(data, start, end, sfreq, event_index):\n",
        "    start_sample = int(event_index + start * sfreq)\n",
        "    end_sample = int(event_index + end * sfreq)\n",
        "    event_data = data[:64, start_sample:end_sample]\n",
        "    plt.imshow(event_data, aspect='auto', extent=[start, end, event_data.shape[0], 0], cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Channels')\n",
        "    plt.ylim(event_data.shape[0]-1, 0)\n",
        "    plt.title('EEG Data Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "# データ選択とヒートマップ作成\n",
        "for fname in bdf_files:\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    sfreq = raw.info['sfreq']  # サンプリングレートの取得\n",
        "    data = raw.get_data()\n",
        "\n",
        "    # Mind-wandering (MW)の最初のイベントを見つけてヒートマップを作成\n",
        "    mw_events = [event[0] for event in events if event[2] == 30]\n",
        "\n",
        "    if mw_events:\n",
        "        create_heatmap(data, -5, 5, sfreq, mw_events[0])  # 最初のMWイベントのヒートマップを作成\n",
        "\n",
        "    # 各イベントインデックスについてヒートマップを生成します。\n",
        "   #for index in mw_events:\n",
        "    # ヒートマップを生成する関数を呼び出します。\n",
        "        #create_heatmap(data, -5, 5, sfreq, index)\n",
        "\n",
        "        # 追加のMWイベントのヒートマップ（オプション）\n",
        "        #for i, event in enumerate(mw_events[1:], start=1):  # 最初のイベントをスキップ\n",
        "            #create_heatmap(data, -5, 5, sfreq, event)  # 次のMWイベントのヒートマップを作成\n",
        "\n",
        "# 最初の列がチャンネル名を含んでいるか確認\n",
        "#ch_names = raw.info['ch_names']\n",
        "#print('The first column contains channel names:', ch_names[-5:])\n",
        "\n",
        "#print(len(mw_events))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EDH6FRSL5o5"
      },
      "outputs": [],
      "source": [
        "#スレッドによる並列化＋タスクの分割(新しい)\n",
        "\n",
        "import mne\n",
        "import time\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def load_data_chunk(fname, start, stop):\n",
        "    #fnameファイルを開き、各チャンクのデータ読み込み\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "    data_chunk = raw.get_data(start=start, stop=stop)\n",
        "    return data_chunk\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "\n",
        "]\n",
        "\n",
        "chunk_size = 1000000  # チャンクサイズの設定\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    future_to_chunk = {}\n",
        "    for fname in bdf_files:\n",
        "        #どのようにチャンクに分割するか決めるために、n_timesを取得する\n",
        "        raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "        for start in range(0, raw.n_times, chunk_size):\n",
        "            future = executor.submit(load_data_chunk, fname, start, start+chunk_size)\n",
        "            future_to_chunk[future] = fname\n",
        "\n",
        "    for future in concurrent.futures.as_completed(future_to_chunk):\n",
        "        fname = future_to_chunk[future]\n",
        "        try:\n",
        "            data_chunk = future.result()\n",
        "        except Exception as exc:\n",
        "            print(f'{fname} generated an exception: {exc}')\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"ファイル読み込みにかかった時間: {elapsed_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvm28zsGc8qx"
      },
      "outputs": [],
      "source": [
        "#異なる時間窓によるデータ分割(これを高速化する！！)\n",
        "import numpy as np\n",
        "import mne\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]  # ファイル名リスト\n",
        "\n",
        "def extract_data_segments(raw_data, events, event_id, start_offset, end_offset):\n",
        "    event_indices = [event[0] for event in events if event[2] == event_id]\n",
        "    segments = []\n",
        "    for index in event_indices:\n",
        "        segment = raw_data[:, index+start_offset:index+end_offset].copy()\n",
        "        segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "sampling_rate = 1024  # サンプリングレート\n",
        "\n",
        "# MWとFの時間窓を定義\n",
        "mw_time_ranges = [\n",
        "    (-10 * sampling_rate, -2 * sampling_rate),\n",
        "]\n",
        "f_time_ranges = [\n",
        "    (0, 8 * sampling_rate),\n",
        "]\n",
        "\n",
        "for fname in bdf_files:\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    data = raw._data[0:64]\n",
        "\n",
        "    for start, end in mw_time_ranges:\n",
        "        mw_segments = extract_data_segments(data, events, 30, start, end)\n",
        "        for i, segment in enumerate(mw_segments):\n",
        "            np.savetxt(f\"MW_{i}.txt\", segment, delimiter=',', header='MW')\n",
        "\n",
        "    for start, end in f_time_ranges:\n",
        "        f_segments = extract_data_segments(data, events, 50, start, end)\n",
        "        for i, segment in enumerate(f_segments):\n",
        "            np.savetxt(f\"F_{i}.txt\", segment, delimiter=',', header='F')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpOIqVYNKYCZ"
      },
      "outputs": [],
      "source": [
        "#並列化処理+データ抽出\n",
        "!pip install mne\n",
        "import numpy as np\n",
        "import mne\n",
        "import time\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]  # ファイル名リスト\n",
        "\n",
        "def extract_data_segments(raw_data, events, event_id, start_offset, end_offset):\n",
        "    event_indices = [event[0] for event in events if event[2] == event_id]\n",
        "    segments = []\n",
        "    for index in event_indices:\n",
        "        segment = raw_data[:, index+start_offset:index+end_offset].copy()\n",
        "        segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "def load_data_chunk(fname, start, stop):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)  # データをディスクに取得\n",
        "    data_chunk = raw.get_data(start=start, stop=stop)  # 指定範囲のデータを取得\n",
        "    events = mne.find_events(raw, initial_event=True)  # イベントを抽出\n",
        "    return data_chunk, events\n",
        "\n",
        "def process_file(fname, mw_time_ranges, f_time_ranges):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    data = raw._data[0:64]\n",
        "    mw_count = 0  # MWファイルのカウント\n",
        "    f_count = 0  # Fファイルのカウント\n",
        "\n",
        "    for start, end in mw_time_ranges:\n",
        "        mw_segments = extract_data_segments(data, events, 30, start, end)\n",
        "        for i, segment in enumerate(mw_segments):\n",
        "            np.savetxt(f\"MW_{i}.txt\", segment, delimiter=',', header='MW')\n",
        "            mw_count += 1  # MWファイルのカウントを増やす\n",
        "\n",
        "    for start, end in f_time_ranges:\n",
        "        f_segments = extract_data_segments(data, events, 50, start, end)\n",
        "        for i, segment in enumerate(f_segments):\n",
        "            np.savetxt(f\"F_{i}.txt\", segment, delimiter=',', header='F')\n",
        "            f_count += 1  # Fファイルのカウントを増やす\n",
        "    return mw_count, f_count\n",
        "\n",
        "chunk_size = 500000\n",
        "#start_time = time.time()\n",
        "\n",
        "sampling_rate = 1024  # サンプリングレート\n",
        "\n",
        "# MWとFの時間窓を定義\n",
        "mw_time_ranges = [\n",
        "    (-10 * sampling_rate, -2 * sampling_rate),  # MWの時間窓\n",
        "]\n",
        "f_time_ranges = [\n",
        "    (0, 8 * sampling_rate),  # Fの時間窓\n",
        "]\n",
        "\n",
        "mw_total_count = 0 #MWファイルのカウント\n",
        "f_total_count = 0 #Fファイルのカウント\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    file_n_times = [(fname, mne.io.read_raw_bdf(fname, preload=False).n_times) for fname in bdf_files]\n",
        "    results = list(executor.map(lambda f: process_file(f[0], mw_time_ranges, f_time_ranges), file_n_times))\n",
        "\n",
        "#elapsed_time = time.time() - start_time\n",
        "#print(f\"ファイル読み込みにかかった時間: {elapsed_time:.2f} 秒\")\n",
        "\n",
        "for mw_count, f_count in results:\n",
        "    mw_total_count += mw_count\n",
        "    f_total_count += f_count\n",
        "\n",
        "print(f\"Total MW files saved: {mw_total_count}\")\n",
        "print(f\"Total F files saved: {f_total_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbOr9MIL03jk"
      },
      "outputs": [],
      "source": [
        "#txtのリストをシャッフルする\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(\"/content/A.csv\")\n",
        "\n",
        "# データフレームをシャッフルする\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# シャッフルされたデータフレームを新しいCSVとして保存する\n",
        "df.to_csv(\"/content/A.csv\", index=False)\n",
        "\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DulXgRiOQPUl"
      },
      "outputs": [],
      "source": [
        "#DATA SELECTION(EEGデータの選択)\n",
        "!pip install mne\n",
        "import mne\n",
        "import numpy as np\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "\n",
        "mw_count = 0\n",
        "f_count = 0\n",
        "\n",
        "for fname in bdf_files:\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    info = raw.info\n",
        "    data = raw._data[0:64]\n",
        "\n",
        "    # Mind-wandering (MW)\n",
        "    mw = [event[0] for event in events if event[2] == 30]\n",
        "    shape_myarray = (64, data.shape[1])\n",
        "    mw_duration = np.zeros(shape_myarray)\n",
        "\n",
        "    for j in range(len(mw)):\n",
        "        mw_duration = data[:, mw[j]-(10*1024):mw[j]-(2*1024)].copy()\n",
        "        np.savetxt(\"MW_\" + str(mw_count) + \".txt\", mw_duration, delimiter=',', header='MW')\n",
        "        mw_count += 1\n",
        "\n",
        "    # Focusing (F)\n",
        "    f = [event[0] for event in events if event[2] == 50]\n",
        "    shape_myarray = (64, data.shape[1])\n",
        "    f_duration = np.zeros(shape_myarray)\n",
        "\n",
        "    for j in range(len(f)):\n",
        "        f_duration = data[:, f[j]:(f[j]+(8*1024))].copy()\n",
        "        np.savetxt(\"F_\" + str(f_count) + \".txt\", f_duration, delimiter=',', header='F')\n",
        "        f_count += 1\n",
        "\n",
        "print(f\"Total F files saved: {f_count}\")\n",
        "print(f\"Total MW files saved: {mw_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks2W7qVpubK_"
      },
      "outputs": [],
      "source": [
        "#Random Forest/SVMを使ったMW識別\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import scipy.signal\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import KFold, cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.signal import welch\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\"\"\"\n",
        "def extract_feature(data):\n",
        "  # 既存の特徴量\n",
        "    mean_signal = np.mean(data, axis=1)\n",
        "    std_signal = np.std(data, axis=1)\n",
        "    max_signal = np.max(data, axis=1)\n",
        "    min_signal = np.min(data, axis=1)\n",
        "    # 新たな特徴量（パワースペクトル密度）\n",
        "    freqs, power = welch(data, fs=1024, nperseg=1024)\n",
        "    theta_band = np.mean(power[:, (freqs >= 4) & (freqs <= 8)], axis=1)\n",
        "    alpha_band = np.mean(power[:, (freqs >= 8) & (freqs <= 12)], axis=1)\n",
        "    beta_band = np.mean(power[:, (freqs >= 13) & (freqs <= 30)], axis=1)\n",
        "    delta_band = np.mean(power[:, (freqs >= 1) & (freqs <= 4)], axis=1)\n",
        "    return np.concatenate((mean_signal, std_signal, max_signal, min_signal, theta_band, alpha_band, beta_band, delta_band), axis=None)\n",
        "\n",
        "def extract_feature(data):\n",
        "    # 短い時間窓の特徴\n",
        "    mean_signal_short = np.mean(data[:, :2048], axis=1)\n",
        "    std_signal_short = np.std(data[:, :2048], axis=1)\n",
        "\n",
        "    # 追加周波数帯域特徴\n",
        "    freqs, power = welch(data, fs=1024, nperseg=1024)\n",
        "    beta_band = np.mean(power[:, (freqs >= 13) & (freqs <= 30)], axis=1)\n",
        "    delta_band = np.mean(power[:, (freqs >= 1) & (freqs <= 4)], axis=1)\n",
        "\n",
        "    return np.concatenate((mean_signal_short, std_signal_short, beta_band, delta_band), axis=None)\n",
        "\"\"\"\n",
        "def extract_feature(data):\n",
        "    # 既存の特徴量\n",
        "    mean_signal = np.mean(data, axis=1)\n",
        "    std_signal = np.std(data, axis=1)\n",
        "    max_signal = np.max(data, axis=1)\n",
        "    min_signal = np.min(data, axis=1)\n",
        "\n",
        "    # 新たな特徴量（周波数帯域）\n",
        "    freqs, power = welch(data, fs=1024, nperseg=1024)\n",
        "    theta_band = np.mean(power[:, (freqs >= 4) & (freqs <= 8)], axis=1)\n",
        "    alpha_band = np.mean(power[:, (freqs >= 8) & (freqs <= 12)], axis=1)\n",
        "    beta_band = np.mean(power[:, (freqs >= 13) & (freqs <= 30)], axis=1)\n",
        "    delta_band = np.mean(power[:, (freqs >= 1) & (freqs <= 4)], axis=1)\n",
        "\n",
        "    # 2048分割\n",
        "    mean_signal_8 = np.mean(data[:, :2048], axis=1)\n",
        "    std_signal_8 = np.std(data[:, :2048], axis=1)\n",
        "    mean_signal_9 = np.mean(data[:, 2048:4096], axis=1)\n",
        "    std_signal_9 = np.std(data[:, 2048:4096], axis=1)\n",
        "    mean_signal_10 = np.mean(data[:, 4096:6144], axis=1)\n",
        "    std_signal_10 = np.std(data[:, 4096:6144], axis=1)\n",
        "    mean_signal_11 = np.mean(data[:, 6144:], axis=1)\n",
        "    std_signal_11 = np.std(data[:, 6144:], axis=1)\n",
        "\n",
        "    # 半分に\n",
        "    mean_signal_13 = np.mean(data[:, :4096], axis=1)\n",
        "    std_signal_13 = np.std(data[:, :4096], axis=1)\n",
        "    mean_signal_14 = np.mean(data[:, 4096:], axis=1)\n",
        "    std_signal_14 = np.std(data[:, 4096:], axis=1)\n",
        "    # 新たな特徴量（例：異なる時間範囲の特徴）\n",
        "    #1024分割\n",
        "    mean_signal_0 = np.mean(data[:, :1024], axis=1)\n",
        "    std_signal_0 = np.std(data[:, :1024], axis=1)\n",
        "    mean_signal_1 = np.mean(data[:, 1024:2048], axis=1)\n",
        "    std_signal_1 = np.std(data[:, 1024:2048], axis=1)\n",
        "    mean_signal_2 = np.mean(data[:, 2048:3072], axis=1)\n",
        "    std_signal_2 = np.std(data[:, 2048:3072], axis=1)\n",
        "    mean_signal_3 = np.mean(data[:, 3072:4096], axis=1)\n",
        "    std_signal_3 = np.std(data[:, 3072:4096], axis=1)\n",
        "    mean_signal_4 = np.mean(data[:, 4096:5120], axis=1)\n",
        "    std_signal_4 = np.std(data[:, 4096:5120], axis=1)\n",
        "    mean_signal_5 = np.mean(data[:, 5120:6144], axis=1)\n",
        "    std_signal_5 = np.std(data[:, 5120:6144], axis=1)\n",
        "    mean_signal_6 = np.mean(data[:, 6144:7168], axis=1)\n",
        "    std_signal_6 = np.std(data[:, 6144:7168], axis=1)\n",
        "    mean_signal_7 = np.mean(data[:, 7168:], axis=1)\n",
        "    std_signal_7 = np.std(data[:, 7168:], axis=1)\n",
        "\n",
        "    # 特徴量を結合\n",
        "    return np.concatenate((mean_signal, std_signal, max_signal, min_signal, theta_band, alpha_band, beta_band, delta_band, mean_signal_0, std_signal_0, mean_signal_1, std_signal_1, mean_signal_2, std_signal_2, mean_signal_3, std_signal_3, mean_signal_4, std_signal_4,mean_signal_5, std_signal_5,mean_signal_6, std_signal_6,mean_signal_7, std_signal_7,mean_signal_8, std_signal_8,mean_signal_9, std_signal_9,mean_signal_10, std_signal_10,mean_signal_11, std_signal_11,mean_signal_13, std_signal_13,mean_signal_14, std_signal_14), axis=None)\n",
        "\n",
        "# MWとFのファイルを取得\n",
        "mw_files = glob.glob(\"/content/MW_*.txt\")\n",
        "f_files = glob.glob(\"/content/F_*.txt\")\n",
        "\n",
        "# 特徴とラベルのリストを作成\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# MWのデータから特徴を抽出\n",
        "for fname in mw_files:\n",
        "    data = np.loadtxt(fname, delimiter=',')\n",
        "    feature = extract_feature(data)\n",
        "    features.append(feature)\n",
        "    #features.append(data)  # データを平坦化して追加\n",
        "    labels.append(1)  # MWのラベル\n",
        "\n",
        "# Fのデータから特徴を抽出\n",
        "for fname in f_files:\n",
        "    data = np.loadtxt(fname, delimiter=',')\n",
        "    feature = extract_feature(data)\n",
        "    features.append(feature)\n",
        "    #features.append(data)  # データを平坦化して追加\n",
        "    labels.append(0)  # Fのラベル\n",
        "\n",
        "# 特徴とラベルをNumPy配列に変換\n",
        "X = np.array(features)\n",
        "y = np.array(labels)\n",
        "\n",
        "# PCAによる次元削減\n",
        "#pca = PCA(n_components=60)\n",
        "#X_pca = pca.fit_transform(X)\n",
        "\n",
        "# 追加: データのバランスを取るため、Fのサンプル数をMWの数に合わせる\n",
        "#mw_sample_indices = np.where(y == 1)[0]  # MWのインデックス\n",
        "#f_sample_indices = np.where(y == 0)[0]  # Fのインデックス\n",
        "mw_sample_indices = np.random.choice(np.where(y == 1)[0], 250, replace=False)\n",
        "f_sample_indices = np.random.choice(np.where(y == 0)[0], 250, replace=False)\n",
        "#f_sample_indices = np.random.choice(f_sample_indices, len(mw_sample_indices), replace=False)  # ランダムに選択\n",
        "f_sample_indices = f_sample_indices[:len(mw_sample_indices)]  # MWのサンプル数=Fのサンプル数に\n",
        "\n",
        "# 新しいバランスの取れたデータセット\n",
        "balanced_indices = np.concatenate([mw_sample_indices, f_sample_indices])\n",
        "X_balanced = X[balanced_indices]\n",
        "y_balanced = y[balanced_indices]\n",
        "\n",
        "# データにNaNが含まれているか確認\n",
        "nan_in_data = np.isnan(X_balanced).any()\n",
        "\n",
        "# NaNがある場合、それを処理\n",
        "if nan_in_data:\n",
        "    # NaN値を列の平均で置き換える例\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    X_balanced = imputer.fit_transform(X_balanced)\n",
        "\n",
        "    # あるいは、NaNを含む行を削除する方法もありますが、これによりデータ量が減少する可能性があります。\n",
        "\n",
        "\n",
        "# 既存のコード: バランスの取れたデータで訓練セットとテストセットに分割\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# 性能指標を定義\n",
        "scoring = {'accuracy': 'accuracy',\n",
        "           'precision': 'precision',\n",
        "           'recall': make_scorer(recall_score),\n",
        "           'specificity': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred).ravel()[0] / (confusion_matrix(y_true, y_pred).ravel()[0] + confusion_matrix(y_true, y_pred).ravel()[1]))}\n",
        "\n",
        "# シャッフルされた交差検証を設定\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# RBFカーネルでSVMを実行するための設定\n",
        "#svm = SVC(kernel='rbf', C=0.00001, gamma='auto')\n",
        "\n",
        "# 交差検証でのパフォーマンスを評価\n",
        "#scores = cross_validate(svm, X_balanced, y_balanced, scoring=scoring, cv=10, return_train_score=True)\n",
        "\n",
        "# 交差検証の実行(多項式カーネル)\n",
        "#scores = cross_validate(SVC(kernel='poly', degree=3, C=1.0, gamma='scale'), X_balanced, y_balanced, scoring=scoring, cv=10, return_train_score=True)\n",
        "\n",
        "# ランダムフォレストの設定\n",
        "#rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "#rf = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
        "#rf = RandomForestClassifier(n_estimators=39, max_depth=7, min_samples_split=7, min_samples_leaf=9, random_state=42)\n",
        "\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 10, 30)\n",
        "    max_depth = trial.suggest_categorical('max_depth', [None, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "                                random_state=42)\n",
        "    scores = cross_val_score(rf, X_balanced, y_balanced, cv=5, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print(\"Best parameters:\", study.best_params)\n",
        "print(\"Best accuracy:\", study.best_value)\n",
        "\n",
        "# Optunaで得られたベストパラメータ\n",
        "best_params = study.best_params\n",
        "\n",
        "# ベストパラメータを使ってランダムフォレストモデルを作成\n",
        "rf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
        "                            max_depth=best_params['max_depth'],\n",
        "                            min_samples_split=best_params['min_samples_split'],\n",
        "                            min_samples_leaf=best_params['min_samples_leaf'],\n",
        "                            random_state=42)\n",
        "\n",
        "# 交差検証でのパフォーマンスを評価\n",
        "scores = cross_validate(rf, X_balanced, y_balanced, scoring=scoring, cv=10, return_train_score=True)\n",
        "\n",
        "# 各フォールドでのテストデータの精度を表示\n",
        "for i in range(10):\n",
        "    print(f\"Fold {i+1}\")\n",
        "    print(f\"Train Accuracy: {scores['train_accuracy'][i] * 100:.2f}%\")\n",
        "    print(f\"Test Accuracy: {scores['test_accuracy'][i] * 100:.2f}%\")\n",
        "\n",
        "# 平均精度と他の性能指標の計算\n",
        "train_accuracy = np.mean(scores['train_accuracy']) * 100\n",
        "test_accuracy = np.mean(scores['test_accuracy']) * 100\n",
        "precision = np.mean(scores['test_precision']) * 100\n",
        "recall = np.mean(scores['test_recall']) * 100\n",
        "specificity = np.mean(scores['test_specificity']) * 100\n",
        "\n",
        "#結果の出力\n",
        "print(f\"\\nTrain Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"Specificity: {specificity:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaAP0Lh7WtH6"
      },
      "outputs": [],
      "source": [
        "#データ出力\n",
        "print(data.shape)\n",
        "print(feature.shape)\n",
        "s = np.std(data[:6, :2096], axis=1)\n",
        "print(s)\n",
        "print(X.shape)\n",
        "print(X_balanced.shape)\n",
        "print(y_balanced.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgwKK0UTDzIu"
      },
      "outputs": [],
      "source": [
        "#optunaを使ったハイパーパラメータ最適化(Random Forest)\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
        "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
        "    scores = cross_val_score(rf, X_balanced, y_balanced, cv=5, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Best parameters:\", study.best_params)\n",
        "print(\"Best accuracy:\", study.best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A2qc-JXwQmz"
      },
      "outputs": [],
      "source": [
        "#Grid searchによるハイパーパラメータ最適化(Random Forest)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# パラメータグリッドの設定\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# グリッドサーチの設定\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=10, scoring='accuracy')\n",
        "\n",
        "# グリッドサーチの実行\n",
        "grid_search.fit(X_balanced, y_balanced)\n",
        "\n",
        "# 最適なパラメータとスコアの表示\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best recall:\", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBCJUFCsjFix",
        "outputId": "62448af1-1d4d-4929-eaeb-4080c83b3bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'C': 1e-09, 'gamma': 1.0}\n",
            "Best accuracy: 0.5185214739851837\n"
          ]
        }
      ],
      "source": [
        "#Grid searchによるハイパーパラメータ最適化(SVM)\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Grid Search用のパラメータグリッドを設定\n",
        "param_grid = {\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
        "    'C': [0.000000001, 0.000001,  0.00001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 4, 5]\n",
        "}\n",
        "\"\"\"\n",
        "def custom_score(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    return min(precision, recall)  # PrecisionとRecallの小さい方をスコアとして返す\n",
        "\n",
        "# GridSearchCVでの使用例\n",
        "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=10, scoring=make_scorer(custom_score))\n",
        "\"\"\"\n",
        "# RBFカーネル用のGrid Searchの実行\n",
        "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=10, scoring='recall')\n",
        "grid_search.fit(X_balanced, y_balanced)\n",
        "\n",
        "# 最適なパラメータの表示\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best accuracy:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJVnKO0582cN"
      },
      "outputs": [],
      "source": [
        "#CNNを使ったMW識別\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from numpy.core.multiarray import asarray\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import f1_score\n",
        "!pip install scikit-optimize\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch.nn.init as init\n",
        "import random\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import torch.optim as optim\n",
        "#正解率などを計算\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "#!pip install optuna[visualization]\n",
        "#import torchmetrics\n",
        "#91.78% accuracy, 92.84% sensitivity(recall), and 90.73% specificity.\n",
        "\n",
        "#drop0.5814981354327653 lr 0.0003952902573873088\n",
        "\n",
        "#ハイパーパラメータ\n",
        "device = torch.device(\"cuda\")\n",
        "num_epochs = 20\n",
        "num_classes = 2\n",
        "batch_size = 13\n",
        "learning_rate = 0.001\n",
        "\n",
        "def time_shift(data, shift_max = 1000):\n",
        "  shift = random.randint(-shift_max, shift_max)\n",
        "  return np.roll(data, shift, axis = 1)\n",
        "\n",
        "def add_noise(data, noise_level=0.01):\n",
        "  noise = np.random.normal(0, noise_level, data.shape)\n",
        "  return data + noise\n",
        "\n",
        "def apply_scaling(data, scale_min=0.8, scale_max=1.2):\n",
        "    scales = np.random.uniform(scale_min, scale_max, data.shape[0])\n",
        "    scaled_data = data * scales[:, np.newaxis]\n",
        "    return scaled_data\n",
        "\n",
        "class CustomDatasetFromCSV(Dataset):\n",
        "  def __init__(self, csv_path, a, b, c, d, length, fold):\n",
        "    self.fold = fold\n",
        "    self.transforms = transforms\n",
        "    self.to_tensor = transforms.ToTensor()\n",
        "    self.data_info = pd.read_csv(csv_path)\n",
        "    if c == 0:\n",
        "      self.image_arr = np.asarray(self.data_info.iloc[a:b, 0])\n",
        "      self.label_arr = np.asarray(self.data_info.iloc[a:b, 1])\n",
        "    else:\n",
        "      self.image_arr = np.concatenate((np.asarray(self.data_info.iloc[a:b, 0]), np.asarray(self.data_info.iloc[c:d, 0])))\n",
        "      self.label_arr = np.concatenate((np.asarray(self.data_info.iloc[a:b, 1]), np.asarray(self.data_info.iloc[c:d, 1])))\n",
        "\n",
        "    #print(f\"Image Array for fold {self.fold}: \", self.image_arr)\n",
        "    print(f\"Fold: {self.fold}\")\n",
        "\n",
        "    self.image_len = length\n",
        "    #print(\"index\", self.image_len)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    single_image_name = self.image_arr[index]\n",
        "\n",
        "    # ここでファイル名を出力（デバッグ用）\n",
        "    #print(f\"Reading data from {single_image_name}.txt\")\n",
        "\n",
        "    lines_img = loadtxt(single_image_name + \".txt\", delimiter=\",\", unpack=False)\n",
        "\n",
        "    # Debugging print statements\n",
        "    #print(\"Shape of lines_img before reshape:\", lines_img.shape)\n",
        "\n",
        "    #データ拡張の適用\n",
        "    #lines_img = time_shift(lines_img) #時間的シフト\n",
        "    #lines_img = add_noise(lines_img) #ノイズ追加\n",
        "    #lines_img = apply_scaling(lines_img)\n",
        "\n",
        "    pad_size = 10240 - lines_img.shape[1]\n",
        "    padded_lines_img = np.pad(lines_img, [(0,0), (0, pad_size)], \"constant\")\n",
        "    padded_lines_img = np.reshape(padded_lines_img, (1, 64, 10240))\n",
        "    img_as_tensor = torch.FloatTensor(padded_lines_img)\n",
        "\n",
        "    #lines_img = np.reshape(lines_img, (1,64,10240))\n",
        "\n",
        "    single_image_label = self.label_arr[index]\n",
        "    single_image_label = torch.LongTensor([single_image_label])\n",
        "    #single_image_label = torch.tensor([single_image_label], dtype = torch.long)\n",
        "    return (img_as_tensor, single_image_label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.image_len\n",
        "\n",
        "\"\"\"\n",
        "class CustomDatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, a, b, c, d, length, fold):\n",
        "        self.fold = fold\n",
        "        self.data_info = pd.read_csv(csv_path)\n",
        "        self.preloaded_data = []\n",
        "        self.label_arr = []\n",
        "\n",
        "        # データのパスを取得\n",
        "        if c == 0:\n",
        "            image_paths = np.asarray(self.data_info.iloc[a:b, 0])\n",
        "            self.label_arr = np.asarray(self.data_info.iloc[a:b, 1])\n",
        "        else:\n",
        "            image_paths = np.concatenate((np.asarray(self.data_info.iloc[a:b, 0]), np.asarray(self.data_info.iloc[c:d, 0])))\n",
        "            self.label_arr = np.concatenate((np.asarray(self.data_info.iloc[a:b, 1]), np.asarray(self.data_info.iloc[c:d, 1])))\n",
        "\n",
        "        # すべてのデータを一度にメモリにロード\n",
        "        for img_path in image_paths:\n",
        "            lines_img = loadtxt(img_path + \".txt\", delimiter=\",\", unpack=False)\n",
        "            lines_img = time_shift(lines_img)\n",
        "            lines_img = add_noise(lines_img)\n",
        "            pad_size = 10240 - lines_img.shape[1]\n",
        "            padded_lines_img = np.pad(lines_img, [(0,0), (0, pad_size)], \"constant\")\n",
        "            padded_lines_img = np.reshape(padded_lines_img, (1, 64, 10240))\n",
        "            self.preloaded_data.append(padded_lines_img)\n",
        "\n",
        "        self.preloaded_data = np.array(self.preloaded_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # メモリにロードされたデータを直接使用\n",
        "        img_as_tensor = torch.FloatTensor(self.preloaded_data[index])\n",
        "        single_image_label = torch.tensor([self.label_arr[index]], dtype=torch.long)\n",
        "        return (img_as_tensor, single_image_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.preloaded_data)\n",
        "\"\"\"\n",
        "\n",
        "#CNNモデルの定義\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, (1, 11)),\n",
        "        nn.Conv2d(32, 32, (64,1)),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(1,2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(32, 20, (1, 10)),\n",
        "        nn.BatchNorm2d(20),\n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(1, 4)\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Conv2d(20, 20, (1, 10)),\n",
        "        nn.BatchNorm2d(20),\n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(1,4)\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Conv2d(20, 20, (1, 11)),\n",
        "        nn.BatchNorm2d(20),\n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(1,3)\n",
        "    )\n",
        "    self.drop_out = nn.Dropout(0.5)\n",
        "    self.fc1 = nn.Linear(2060, 100)\n",
        "    self.fc2 = nn.Linear(100, 2)\n",
        "    #self.fc3 = nn.Linear(50, 2)\n",
        "    self._initialize_weights()\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        init.constant_(m.bias, 0)\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    #print(f\"Before entering fc1, shape of out: {out.shape}\")\n",
        "    out = self.drop_out(out)\n",
        "    #out = F.relu(self.fc1(out))\n",
        "    out = self.fc1(out)\n",
        "    out = self.fc2(out)\n",
        "    #out = F.relu(self.fc2(out))\n",
        "    #out = self.drop_out(out)\n",
        "    #out = self.fc3(out)\n",
        "    return out\n",
        "\n",
        "\"\"\"\n",
        "#optuna用のデータロード\n",
        "optuna_train_dataset = CustomDatasetFromCSV(\"/content/A.csv\", 10, 50, 0, 0, 30, 1)\n",
        "train_loader = torch.utils.data.DataLoader(optuna_train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
        "\n",
        "optuna_val_dataset = CustomDatasetFromCSV(\"/content/A.csv\", 60, 70, 0, 0, 10, 1)\n",
        "val_loader = torch.utils.data.DataLoader(optuna_val_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
        "\n",
        "#optunaによるハイパーパラメータ最適化\n",
        "def objective(trial):\n",
        "  dropout_rate = trial.suggest_float(\"dropout_rate\", 0.3, 0.7)\n",
        "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-2)\n",
        "\n",
        "  model = ConvNet(num_classes, dropout_rate).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay=1e-5)\n",
        "\n",
        "  #モデルの訓練+val\n",
        "  num_epochs=10\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      #print(\"訓練用の正解ラベル = \", labels)\n",
        "      labels = labels.view(-1)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    #検証ループ\n",
        "    val_accuracy = calculate_val_accuracy(model, val_loader)\n",
        "    print(f'Epoch {epoch}: Validation Accuracy = {val_accuracy}%')\n",
        "\n",
        "    #Pruner\n",
        "    try:\n",
        "      trial.report(val_accuracy, epoch)\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "    except optuna.exceptions.TrialPruned:\n",
        "      print(f\"Trial {trial.number} early stopped at epoch {epoch}.\")\n",
        "      raise\n",
        "  return val_accuracy\n",
        "\n",
        "#ハイパーパラメータ最適化のval精度の計算\n",
        "def calculate_val_accuracy(model, val_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            labels = labels.squeeze()\n",
        "            print(f\"Predicted: {predicted}\")\n",
        "            print(f\"Labels: {labels}\")\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print(f\"Total: {total}, Correct: {correct}\")\n",
        "    val_accuracy = 100 * correct / total\n",
        "    return val_accuracy\n",
        "\n",
        "#Pruner\n",
        "pruner = optuna.pruners.MedianPruner(\n",
        "    n_startup_trials=5,  # 最初の2トライアルはプルーニングしない\n",
        "    n_warmup_steps=2,    # 最初からプルーニングのチェックを開始\n",
        "    interval_steps=1     # 1エポックごとにプルーニングのチェックを行う\n",
        ")\n",
        "\n",
        "#optuna実行\n",
        "study = optuna.create_study(direction=\"maximize\", pruner = pruner)\n",
        "study.optimize(objective, n_trials=20)\n",
        "#進捗の可視化\n",
        "#optuna.visualization.plot_optimization_history(study)\n",
        "#最高のハイパーパラメータを設定\n",
        "best_dropout = study.best_params[\"dropout_rate\"]\n",
        "best_lr = study.best_params[\"lr\"]\n",
        "\"\"\"\n",
        "\n",
        "#10-fold交差検証によるモデルの訓練とテスト\n",
        "ns = 14\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "# 各指標の合計値を初期化\n",
        "total_accuracy = 0\n",
        "total_precision = 0\n",
        "total_recall = 0\n",
        "total_f1 = 0\n",
        "total_specificity = 0\n",
        "for fold in range(1, 11):\n",
        "  if fold == 1:\n",
        "    a1_train = 14\n",
        "    a2_train = 144\n",
        "    a1_test = 0\n",
        "    a2_test = 14\n",
        "    custom_mnist_from_csv_train = CustomDatasetFromCSV(\"/content/A.csv\", a1_train, a2_train, 0, 0, 130, fold)\n",
        "    custom_mnist_from_csv_test = CustomDatasetFromCSV(\"/content/A.csv\", a1_test, a2_test, 0, 0, 14, fold)\n",
        "  elif fold == 10:\n",
        "    a1_train = 0\n",
        "    a2_train = 130\n",
        "    a1_test = 130\n",
        "    a2_test = 144\n",
        "    custom_mnist_from_csv_train = CustomDatasetFromCSV(\"/content/A.csv\", a1_train, a2_train, 0, 0, 130, fold)\n",
        "    custom_mnist_from_csv_test = CustomDatasetFromCSV(\"/content/A.csv\", a1_test, a2_test, 0, 0, 14, fold)\n",
        "  else:\n",
        "    a1_train = 0\n",
        "    a2_train = (fold-2)*ns\n",
        "    b1_train = (fold-1)*ns\n",
        "    b2_train = 144\n",
        "    a1_test = a2_train\n",
        "    a2_test = b1_train\n",
        "    custom_mnist_from_csv_train = CustomDatasetFromCSV(\"/content/A.csv\", a1_train, a2_train, b1_train, b2_train, 130, fold)\n",
        "    custom_mnist_from_csv_test = CustomDatasetFromCSV(\"/content/A.csv\", a1_test, a2_test, 0, 0, 14, fold)\n",
        "\n",
        "  #訓練データをロードする(13から10へ)\n",
        "  dataset = torch.utils.data.DataLoader(dataset = custom_mnist_from_csv_train, batch_size=10, shuffle=True, num_workers=8)\n",
        "  #モデルを設定\n",
        "  model = ConvNet(num_classes).to(device)\n",
        "  #model = ConvNet(num_classes, best_dropout).to(device) #optuna\n",
        "  #重み初期化\n",
        "  #model._initialize_weights()\n",
        "  #モデルをCudaに移す\n",
        "  model = model.cuda()\n",
        "\n",
        "  #損失関数設定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #optimizer設定\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
        "  #optimizer = torch.optim.Adam(model.parameters(), lr = best_lr, weight_decay=1e-5)\n",
        "\n",
        "  total_step = len(dataset)\n",
        "\n",
        "  sumtrain = 0\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  best_accuracy = 0.0\n",
        "  patience = 4\n",
        "  counter = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    #print(\"train_accuracy = \", sumtrain)\n",
        "    #print(\"loss = \", loss)\n",
        "\n",
        "    total_train_data = 0\n",
        "    sumtrain = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataset):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      total_train_data += len(labels)\n",
        "\n",
        "      outputs = model(images)\n",
        "      labels = labels.view(-1)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      outputs = torch.cuda.FloatTensor(outputs)\n",
        "      #outputs = torch.tensor(outputs, dtype=torch.float, device='cuda')\n",
        "\n",
        "      correct = 0\n",
        "      for j in range(len(outputs)):\n",
        "        if (outputs[j, 0] < outputs[j, 1]) and torch.eq(labels[j], 1):\n",
        "          correct += 1\n",
        "        elif (outputs[j, 0] > outputs[j, 1]) and torch.eq(labels[j], 0):\n",
        "          correct += 1\n",
        "\n",
        "      sumtrain += correct\n",
        "    #エポックごとの訓練精度\n",
        "    train_accuracy = (sumtrain / total_train_data) * 100\n",
        "    print(f\"Epoch {epoch+1}, Train Accuracy: {train_accuracy: .2f}%\")\n",
        "\n",
        "    #Early Stopping\n",
        "    if train_accuracy > best_accuracy:\n",
        "      best_accuracy = train_accuracy\n",
        "      counter = 0\n",
        "    else:\n",
        "      counter += 1\n",
        "    if counter >= patience:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "  #各foldごとの訓練精度\n",
        "  train_accuracies.append(best_accuracy)\n",
        "\n",
        "  #テストデータロード。num_workersで並列処理をして高速化\n",
        "  dataset = torch.utils.data.DataLoader(dataset=custom_mnist_from_csv_test, batch_size=7, shuffle=True, num_workers=8)\n",
        "#9から７へ\n",
        "  #f1 score用\n",
        "  P_labels = []\n",
        "  T_labels = []\n",
        "  #テスト用のバッチ\n",
        "  for i, (images, labels) in enumerate(dataset):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct += (predicted==labels).sum().item()\n",
        "    P_labels.extend(predicted.cpu().numpy())\n",
        "    T_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  # 追加する指標の計算\n",
        "  total_accuracy += accuracy_score(T_labels, P_labels) * 100\n",
        "  total_precision += precision_score(T_labels, P_labels) * 100\n",
        "  total_recall += recall_score(T_labels, P_labels) * 100\n",
        "  total_f1 += f1_score(T_labels, P_labels)\n",
        "  tn, fp, fn, tp = confusion_matrix(T_labels, P_labels).ravel()\n",
        "  total_specificity += tn / (tn + fp) * 100\n",
        "\n",
        "\n",
        "# 最終的な平均値を計算\n",
        "average_train_accuracy = sum(train_accuracies) / len(train_accuracies)\n",
        "average_accuracy = total_accuracy / 10\n",
        "average_precision = total_precision / 10\n",
        "average_recall = total_recall / 10\n",
        "average_f1 = total_f1 / 10\n",
        "average_specificity = total_specificity / 10\n",
        "\n",
        "# 結果の出力\n",
        "print(f\"Final Train Accuracy = {average_train_accuracy: .2f}%\")\n",
        "print(f\"Accuracy = {average_accuracy: .2f}%\")\n",
        "print(f\"Average Recall = {average_recall: .2f}%\")\n",
        "#print(f\"Average Precision: = {average_precision: .2f}%\")\n",
        "#print(\"Average F1 Score:\", average_f1)\n",
        "#print(\"Average Specificity:\", average_specificity)\n",
        "\n",
        "#出力\n",
        "#print(\"Length of train_accuracies\", len(train_accuracies))\n",
        "#print(f\"Final Train Accuracy = {average_train_accuracy: .2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY6pbDB2-P69"
      },
      "outputs": [],
      "source": [
        "print(f\"Final Train Accuracy = {average_train_accuracy: .2f}%\")\n",
        "print(f\"Accuracy = {average_accuracy: .2f}%\")\n",
        "print(f\"Average Recall = {average_recall: .2f}%\")\n",
        "print(f\"Average Specificity: = {average_specificity: .2f}%\")\n",
        "\n",
        "print(f\"Average Precision: = {average_precision: .2f}%\")\n",
        "print(\"Average F1 Score:\", average_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krs3oZB77cC6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8qXy6bh7cFp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1B-ORbyBXrh7fEMzsv14rG9h0Y5WiSITG",
      "authorship_tag": "ABX9TyMAB+hLvwqGW159WpHNjqtv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}