{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daisuke134/MW_detection/blob/main/MW_DETECTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9savk1u4Fv7V",
        "outputId": "b2a091d2-b24d-4d1b-df6e-53907b385a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.5.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mne) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from mne) (1.11.3)\n",
            "Requirement already satisfied: matplotlib>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (3.11.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.5.1\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 1937407  =      0.000 ...  1891.999 secs...\n",
            "1283 events found\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n"
          ]
        }
      ],
      "source": [
        "#DATA SELECTION\n",
        "!pip install mne\n",
        "import mne\n",
        "import numpy as np\n",
        "\n",
        "#select the address of original dataset\n",
        "fname = \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\"\n",
        "\n",
        "#read the dataset\n",
        "raw = mne.io.read_raw_bdf(fname,preload=True)\n",
        "\n",
        "#save the events of the dataset in a variable\n",
        "events = mne.find_events(raw, initial_event=True)\n",
        "\n",
        "#get the data information\n",
        "info = raw.info\n",
        "\n",
        "#select the first 64 channels(the first 64 channels are EEG data)\n",
        "data = raw._data[0:64]\n",
        "\n",
        "mw = []\n",
        "\n",
        "#extract mind wandering events in dataset\n",
        "for i in range(0, events.shape[0]):\n",
        "    if events[i,2] == 30:\n",
        "        mw.append(events[i,0])\n",
        "\n",
        "shape_myarray=(64,data.shape[1])\n",
        "\n",
        "#define an array of zeros for mind wandering data\n",
        "mw_duration = np.zeros(shape_myarray)\n",
        "\n",
        "#save 8-second mind wandering samples\n",
        "for j in range(0,len(mw)):\n",
        "    mw_duration = data[:,mw[j]-10240:mw[j]-2048].copy()\n",
        "    np.savetxt(\"MW_\" + str(j) + \".txt\", mw_duration, delimiter = ',', header = 'MW')\n",
        "\n",
        "\n",
        "f = []\n",
        "\n",
        "#extract focusing state events in dataset\n",
        "for i in range(0, events.shape[0]):\n",
        "    if events[i,2] == 50:\n",
        "        f.append(events[i,0])\n",
        "\n",
        "shape_myarray=(64,data.shape[1])\n",
        "\n",
        "#define an array of zeros for focusing state data\n",
        "f_duration = np.zeros(shape_myarray)\n",
        "\n",
        "#save 8-second mind wandering samples\n",
        "for j in range(0,len(f)):\n",
        "    f_duration = data[:,f[j]:f[j]+8192].copy()\n",
        "    np.savetxt(\"F_\" + str(j) + \".txt\", f_duration, delimiter = ',', header = 'F')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16BCdoOTlX38"
      },
      "outputs": [],
      "source": [
        "# 複数のチャンクサイズで試す+スレッド並列処理\n",
        "\n",
        "import mne\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def load_data(fname, chunk_size):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "    total_time = 0\n",
        "    for start in range(0, raw.n_times, chunk_size):\n",
        "        start_time = time.time()\n",
        "        data_chunk = raw.get_data(start=start, stop=start+chunk_size)\n",
        "        total_time += time.time() - start_time\n",
        "    return total_time\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]  # ファイルリスト\n",
        "\n",
        "# 複数のチャンクサイズで試す\n",
        "chunk_sizes = [50000, 100000, 150000, 200000, 250000, 300000, 500000, 1000000]\n",
        "best_time = float('inf')\n",
        "best_chunk_size = None\n",
        "\n",
        "for chunk_size in chunk_sizes:\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        times = list(executor.map(lambda fname: load_data(fname, chunk_size), bdf_files))\n",
        "    avg_time = sum(times) / len(times)\n",
        "    if avg_time < best_time:\n",
        "        best_time = avg_time\n",
        "        best_chunk_size = chunk_size\n",
        "\n",
        "print(f\"最適なチャンクサイズ: {best_chunk_size}, 平均読み込み時間: {best_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HytOrlkv9N7d"
      },
      "outputs": [],
      "source": [
        "#スレッドによる並列化＋タスクの分割\n",
        "\n",
        "import mne\n",
        "import time\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def load_data_chunk(fname, start, stop):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "    data_chunk = raw.get_data(start=start, stop=stop)\n",
        "    return data_chunk\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "\n",
        "chunk_size = 1000000  # チャンクサイズの設定\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    future_to_chunk = {executor.submit(load_data_chunk, fname, start, start+chunk_size): fname for fname in bdf_files for start in range(0, raw.n_times, chunk_size)}\n",
        "    for future in concurrent.futures.as_completed(future_to_chunk):\n",
        "        fname = future_to_chunk[future]\n",
        "        try:\n",
        "            data_chunk = future.result()\n",
        "        except Exception as exc:\n",
        "            print(f'{fname} generated an exception: {exc}')\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"ファイル読み込みにかかった時間: {elapsed_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HroMihn082ds"
      },
      "outputs": [],
      "source": [
        "#マルチプロセス\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "import mne\n",
        "import time\n",
        "\n",
        "def process_file(fname, n_times):\n",
        "    def load_data_chunk(start, stop):\n",
        "        raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "        data_chunk = raw.get_data(start=start, stop=stop)\n",
        "        return data_chunk\n",
        "\n",
        "    chunk_size = 1000000\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        start_time = time.time()\n",
        "        chunks = [executor.submit(load_data_chunk, start, start+chunk_size) for start in range(0, n_times, chunk_size)]\n",
        "        results = [future.result() for future in concurrent.futures.as_completed(chunks)]\n",
        "        total_time = time.time() - start_time\n",
        "    return total_time\n",
        "\n",
        "def execute_process_file(args):\n",
        "    return process_file(*args)\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "#(fname)とn_timesのリストを取得\n",
        "file_n_times = [(fname, mne.io.read_raw_bdf(fname, preload=False).n_times) for fname in bdf_files]\n",
        "\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    times = list(executor.map(execute_process_file, file_n_times))\n",
        "\n",
        "avg_time = sum(times) / len(times)\n",
        "print(f\"平均読み込み時間: {avg_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p0wRkyS4Ziav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwZwjm1uZino"
      },
      "outputs": [],
      "source": [
        "#ヒートマップ作成\n",
        "# 必要なライブラリをインストール\n",
        "!pip install mne matplotlib\n",
        "\n",
        "# ライブラリをインポート\n",
        "import mne\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ファイル名リストの例 (適宜修正してください)\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "\n",
        "# ヒートマップを作成する関数\n",
        "def create_heatmap(data, start, end, sfreq, event_index):\n",
        "    start_sample = int(event_index + start * sfreq)\n",
        "    end_sample = int(event_index + end * sfreq)\n",
        "    event_data = data[:64, start_sample:end_sample]\n",
        "    plt.imshow(event_data, aspect='auto', extent=[start, end, event_data.shape[0], 0], cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Channels')\n",
        "    plt.ylim(event_data.shape[0]-1, 0)\n",
        "    plt.title('EEG Data Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "# データ選択とヒートマップ作成\n",
        "for fname in bdf_files:\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    sfreq = raw.info['sfreq']  # サンプリングレートの取得\n",
        "    data = raw.get_data()\n",
        "\n",
        "    # Mind-wandering (MW)の最初のイベントを見つけてヒートマップを作成\n",
        "    mw_events = [event[0] for event in events if event[2] == 30]\n",
        "\n",
        "    if mw_events:\n",
        "        create_heatmap(data, -5, 5, sfreq, mw_events[0])  # 最初のMWイベントのヒートマップを作成\n",
        "\n",
        "    # 各イベントインデックスについてヒートマップを生成します。\n",
        "   #for index in mw_events:\n",
        "    # ヒートマップを生成する関数を呼び出します。\n",
        "        #create_heatmap(data, -5, 5, sfreq, index)\n",
        "\n",
        "        # 追加のMWイベントのヒートマップ（オプション）\n",
        "        #for i, event in enumerate(mw_events[1:], start=1):  # 最初のイベントをスキップ\n",
        "            #create_heatmap(data, -5, 5, sfreq, event)  # 次のMWイベントのヒートマップを作成\n",
        "\n",
        "# 最初の列がチャンネル名を含んでいるか確認\n",
        "#ch_names = raw.info['ch_names']\n",
        "#print('The first column contains channel names:', ch_names[-5:])\n",
        "\n",
        "#print(len(mw_events))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EDH6FRSL5o5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6425b2d2-d3bd-4eac-9c52-cc34722a76be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "BDF file detected\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "ファイル読み込みにかかった時間: 2.79 秒\n"
          ]
        }
      ],
      "source": [
        "#スレッドによる並列化＋タスクの分割(新しい)\n",
        "\n",
        "import mne\n",
        "import time\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def load_data_chunk(fname, start, stop):\n",
        "    #fnameファイルを開き、各チャンクのデータ読み込み\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "    data_chunk = raw.get_data(start=start, stop=stop)\n",
        "    return data_chunk\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "\n",
        "]\n",
        "\n",
        "chunk_size = 1000000  # チャンクサイズの設定\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    future_to_chunk = {}\n",
        "    for fname in bdf_files:\n",
        "        #どのようにチャンクに分割するか決めるために、n_timesを取得する\n",
        "        raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "        for start in range(0, raw.n_times, chunk_size):\n",
        "            future = executor.submit(load_data_chunk, fname, start, start+chunk_size)\n",
        "            future_to_chunk[future] = fname\n",
        "\n",
        "    for future in concurrent.futures.as_completed(future_to_chunk):\n",
        "        fname = future_to_chunk[future]\n",
        "        try:\n",
        "            data_chunk = future.result()\n",
        "        except Exception as exc:\n",
        "            print(f'{fname} generated an exception: {exc}')\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"ファイル読み込みにかかった時間: {elapsed_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvm28zsGc8qx"
      },
      "outputs": [],
      "source": [
        "#異なる時間窓によるデータ分割(これを高速化する！！)\n",
        "import numpy as np\n",
        "import mne\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]  # ファイル名リスト\n",
        "\n",
        "def extract_data_segments(raw_data, events, event_id, start_offset, end_offset):\n",
        "    event_indices = [event[0] for event in events if event[2] == event_id]\n",
        "    segments = []\n",
        "    for index in event_indices:\n",
        "        segment = raw_data[:, index+start_offset:index+end_offset].copy()\n",
        "        segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "sampling_rate = 1024  # サンプリングレート\n",
        "\n",
        "# MWとFの時間窓を定義\n",
        "mw_time_ranges = [\n",
        "    (-10 * sampling_rate, -2 * sampling_rate),\n",
        "]\n",
        "f_time_ranges = [\n",
        "    (0, 8 * sampling_rate),\n",
        "]\n",
        "\n",
        "for fname in bdf_files:\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    data = raw._data[0:64]\n",
        "\n",
        "    for start, end in mw_time_ranges:\n",
        "        mw_segments = extract_data_segments(data, events, 30, start, end)\n",
        "        for i, segment in enumerate(mw_segments):\n",
        "            np.savetxt(f\"MW_{i}.txt\", segment, delimiter=',', header='MW')\n",
        "\n",
        "    for start, end in f_time_ranges:\n",
        "        f_segments = extract_data_segments(data, events, 50, start, end)\n",
        "        for i, segment in enumerate(f_segments):\n",
        "            np.savetxt(f\"F_{i}.txt\", segment, delimiter=',', header='F')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrT5LWloIZTJ"
      },
      "outputs": [],
      "source": [
        "#multi process/multi thread\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "import mne\n",
        "import time\n",
        "\n",
        "def process_file(fname, n_times):\n",
        "    def load_data_chunk(start, stop):\n",
        "        raw = mne.io.read_raw_bdf(fname, preload=False)\n",
        "        data_chunk = raw.get_data(start=start, stop=stop)\n",
        "        return data_chunk\n",
        "\n",
        "    chunk_size = 1000000\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        chunks = [executor.submit(load_data_chunk, start, start+chunk_size) for start in range(0, n_times, chunk_size)]\n",
        "        results = [future.result() for future in concurrent.futures.as_completed(chunks)]\n",
        "    return\n",
        "\n",
        "bdf_files = [\n",
        "]\n",
        "file_n_times = [(fname, mne.io.read_raw_bdf(fname, preload=False).n_times) for fname in bdf_files]\n",
        "\n",
        "start_time = time.time()  # 全体の処理開始時間\n",
        "\"\"\"\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    executor.map(lambda f: process_file(*f), file_n_times)\n",
        "\n",
        "elapsed_time = time.time() - start_time  # 全体の処理時間\n",
        "print(f\"全体の処理時間: {elapsed_time:.2f} 秒\")\n",
        "\"\"\"\n",
        "\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    # 各ファイルの処理を開始\n",
        "    futures = [executor.submit(process_file, fname, n_times) for fname, n_times in file_n_times]\n",
        "    # 全てのファイルの処理が完了するまで待つ\n",
        "    results = [future.result() for future in futures]\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"全体の処理時間: {elapsed_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total MW files saved: {mw_total_count}\")\n",
        "print(f\"Total F files saved: {f_total_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9ZidRRNvqnF",
        "outputId": "2580cc1b-745e-455f-c726-e0b13cb70fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total MW files saved: 472\n",
            "Total F files saved: 511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpOIqVYNKYCZ",
        "outputId": "d8213ea8-c91c-42ad-aa4d-68add99ed717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.5.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mne) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from mne) (1.11.3)\n",
            "Requirement already satisfied: matplotlib>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (4.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.5.1\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf...\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Creating raw.info structure...\n",
            "Setting channel info structure...\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 1937407  =      0.000 ...  1891.999 secs...\n",
            "Reading 0 ... 1836031  =      0.000 ...  1792.999 secs...\n",
            "Reading 0 ... 1645567  =      0.000 ...  1606.999 secs...\n",
            "Reading 0 ... 1843199  =      0.000 ...  1799.999 secs...\n",
            "1262 events found\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "1272 events found\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "1283 events found\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "1250 events found\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "Total MW files saved: 82\n",
            "Total F files saved: 87\n"
          ]
        }
      ],
      "source": [
        "#並列化処理+データ抽出\n",
        "!pip install mne\n",
        "import numpy as np\n",
        "import mne\n",
        "import time\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "bdf_files = [\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-01_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-3_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-4_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-5_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-6_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-7_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-8_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-9_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "    \"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]  # ファイル名リスト\n",
        "\n",
        "def extract_data_segments(raw_data, events, event_id, start_offset, end_offset):\n",
        "    event_indices = [event[0] for event in events if event[2] == event_id]\n",
        "    segments = []\n",
        "    for index in event_indices:\n",
        "        segment = raw_data[:, index+start_offset:index+end_offset].copy()\n",
        "        segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "def load_data_chunk(fname, start, stop):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=False)  # データをディスクに取得\n",
        "    data_chunk = raw.get_data(start=start, stop=stop)  # 指定範囲のデータを取得\n",
        "    events = mne.find_events(raw, initial_event=True)  # イベントを抽出\n",
        "    return data_chunk, events\n",
        "\n",
        "def process_file(fname, mw_time_ranges, f_time_ranges):\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    data = raw._data[0:64]\n",
        "    mw_count = 0  # MWファイルのカウント\n",
        "    f_count = 0  # Fファイルのカウント\n",
        "\n",
        "    for start, end in mw_time_ranges:\n",
        "        mw_segments = extract_data_segments(data, events, 30, start, end)\n",
        "        for i, segment in enumerate(mw_segments):\n",
        "            np.savetxt(f\"MW_{i}.txt\", segment, delimiter=',', header='MW')\n",
        "            mw_count += 1  # MWファイルのカウントを増やす\n",
        "\n",
        "    for start, end in f_time_ranges:\n",
        "        f_segments = extract_data_segments(data, events, 50, start, end)\n",
        "        for i, segment in enumerate(f_segments):\n",
        "            np.savetxt(f\"F_{i}.txt\", segment, delimiter=',', header='F')\n",
        "            f_count += 1  # Fファイルのカウントを増やす\n",
        "    return mw_count, f_count\n",
        "\n",
        "chunk_size = 500000\n",
        "#start_time = time.time()\n",
        "\n",
        "sampling_rate = 1024  # サンプリングレート\n",
        "\n",
        "# MWとFの時間窓を定義\n",
        "mw_time_ranges = [\n",
        "    (-10 * sampling_rate, -2 * sampling_rate),  # MWの時間窓\n",
        "]\n",
        "f_time_ranges = [\n",
        "    (0, 8 * sampling_rate),  # Fの時間窓\n",
        "]\n",
        "\n",
        "mw_total_count = 0 #MWファイルのカウント\n",
        "f_total_count = 0 #Fファイルのカウント\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    file_n_times = [(fname, mne.io.read_raw_bdf(fname, preload=False).n_times) for fname in bdf_files]\n",
        "    results = list(executor.map(lambda f: process_file(f[0], mw_time_ranges, f_time_ranges), file_n_times))\n",
        "\n",
        "#elapsed_time = time.time() - start_time\n",
        "#print(f\"ファイル読み込みにかかった時間: {elapsed_time:.2f} 秒\")\n",
        "\n",
        "for mw_count, f_count in results:\n",
        "    mw_total_count += mw_count\n",
        "    f_total_count += f_count\n",
        "\n",
        "print(f\"Total MW files saved: {mw_total_count}\")\n",
        "print(f\"Total F files saved: {f_total_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "UbOr9MIL03jk",
        "outputId": "7b6dbb58-3cf8-4765-e2e7-0c829a2d390f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Sample  Label\n",
              "0   F_45      0\n",
              "1  MW_19      1\n",
              "2   F_10      0\n",
              "3   F_25      0\n",
              "4  MW_56      1\n",
              "5  MW_12      1\n",
              "6   F_60      0\n",
              "7  MW_65      1\n",
              "8  MW_66      1\n",
              "9  MW_18      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b40cef01-07e6-4118-b224-b845b6c31538\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sample</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F_45</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MW_19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>F_25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MW_56</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MW_12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>F_60</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MW_65</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MW_66</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>MW_18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b40cef01-07e6-4118-b224-b845b6c31538')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b40cef01-07e6-4118-b224-b845b6c31538 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b40cef01-07e6-4118-b224-b845b6c31538');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cec2006a-8a68-405d-a1db-2002ddd8e6fc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cec2006a-8a68-405d-a1db-2002ddd8e6fc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cec2006a-8a68-405d-a1db-2002ddd8e6fc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#txtのリストをシャッフルする\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(\"/content/A.csv\")\n",
        "\n",
        "# データフレームをシャッフルする\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# シャッフルされたデータフレームを新しいCSVとして保存する\n",
        "df.to_csv(\"/content/A.csv\", index=False)\n",
        "\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DulXgRiOQPUl",
        "outputId": "d71919cc-c19f-4b9b-cc7a-fa5beb56e683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.6.0-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from mne) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from mne) (1.11.3)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from mne) (0.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (4.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.6.0\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 1937407  =      0.000 ...  1891.999 secs...\n",
            "1283 events found on stim channel Status\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 1836031  =      0.000 ...  1792.999 secs...\n",
            "1262 events found on stim channel Status\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 1645567  =      0.000 ...  1606.999 secs...\n",
            "1250 events found on stim channel Status\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "Extracting EDF parameters from /content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf...\n",
            "BDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Reading 0 ... 1843199  =      0.000 ...  1799.999 secs...\n",
            "1272 events found on stim channel Status\n",
            "Event IDs: [   10    20    21    30    31    50 65536]\n",
            "Total F files saved: 87\n",
            "Total MW files saved: 82\n"
          ]
        }
      ],
      "source": [
        "#DATA SELECTION\n",
        "!pip install mne\n",
        "import mne\n",
        "import numpy as np\n",
        "\n",
        "bdf_files = [\n",
        "\"/content/drive/MyDrive/BDF/sub-01_ses-1_task-BreathCounting_eeg.bdf\",\n",
        "\"/content/drive/MyDrive/BDF/sub-01_ses-2_task-BreathCounting_eeg.bdf\",\n",
        "\"/content/drive/MyDrive/BDF/sub-02_ses-10_task-BreathCounting_eeg.bdf\",\n",
        "\"/content/drive/MyDrive/BDF/sub-02_ses-11_task-BreathCounting_eeg.bdf\",\n",
        "]\n",
        "\n",
        "mw_count = 0\n",
        "f_count = 0\n",
        "\n",
        "for fname in bdf_files:\n",
        "    raw = mne.io.read_raw_bdf(fname, preload=True)\n",
        "    events = mne.find_events(raw, initial_event=True)\n",
        "    info = raw.info\n",
        "    data = raw._data[0:64]\n",
        "\n",
        "    # Mind-wandering (MW)\n",
        "    mw = [event[0] for event in events if event[2] == 30]\n",
        "    shape_myarray = (64, data.shape[1])\n",
        "    mw_duration = np.zeros(shape_myarray)\n",
        "\n",
        "    for j in range(len(mw)):\n",
        "        mw_duration = data[:, mw[j]-(10*1024):mw[j]-(2*1024)].copy()\n",
        "        np.savetxt(\"MW_\" + str(mw_count) + \".txt\", mw_duration, delimiter=',', header='MW')\n",
        "        mw_count += 1\n",
        "\n",
        "    # Focusing (F)\n",
        "    f = [event[0] for event in events if event[2] == 50]\n",
        "    shape_myarray = (64, data.shape[1])\n",
        "    f_duration = np.zeros(shape_myarray)\n",
        "\n",
        "    for j in range(len(f)):\n",
        "        f_duration = data[:, f[j]:(f[j]+(8*1024))].copy()\n",
        "        np.savetxt(\"F_\" + str(f_count) + \".txt\", f_duration, delimiter=',', header='F')\n",
        "        f_count += 1\n",
        "\n",
        "print(f\"Total F files saved: {f_count}\")\n",
        "print(f\"Total MW files saved: {mw_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJVnKO0582cN",
        "outputId": "45f9fea4-3ade-4a6d-c736-cf3ab838b543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (23.9.7)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.12.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Fold: 1\n",
            "Fold: 1\n",
            "Epoch 1, Train Accuracy:  50.77%\n",
            "Epoch 2, Train Accuracy:  47.69%\n",
            "Epoch 3, Train Accuracy:  56.15%\n",
            "Epoch 4, Train Accuracy:  49.23%\n",
            "Epoch 5, Train Accuracy:  44.62%\n",
            "Epoch 6, Train Accuracy:  50.77%\n",
            "Epoch 7, Train Accuracy:  52.31%\n",
            "Early stopping\n",
            "Fold: 2\n",
            "Fold: 2\n",
            "Epoch 1, Train Accuracy:  41.54%\n",
            "Epoch 2, Train Accuracy:  49.23%\n",
            "Epoch 3, Train Accuracy:  56.15%\n",
            "Epoch 4, Train Accuracy:  53.85%\n",
            "Epoch 5, Train Accuracy:  52.31%\n",
            "Epoch 6, Train Accuracy:  48.46%\n",
            "Epoch 7, Train Accuracy:  46.92%\n",
            "Early stopping\n",
            "Fold: 3\n",
            "Fold: 3\n",
            "Epoch 1, Train Accuracy:  51.54%\n",
            "Epoch 2, Train Accuracy:  50.00%\n",
            "Epoch 3, Train Accuracy:  43.85%\n",
            "Epoch 4, Train Accuracy:  47.69%\n",
            "Epoch 5, Train Accuracy:  52.31%\n",
            "Epoch 6, Train Accuracy:  43.08%\n",
            "Epoch 7, Train Accuracy:  46.15%\n",
            "Epoch 8, Train Accuracy:  50.77%\n",
            "Epoch 9, Train Accuracy:  43.08%\n",
            "Early stopping\n",
            "Fold: 4\n",
            "Fold: 4\n",
            "Epoch 1, Train Accuracy:  50.00%\n",
            "Epoch 2, Train Accuracy:  50.77%\n",
            "Epoch 3, Train Accuracy:  51.54%\n",
            "Epoch 4, Train Accuracy:  43.85%\n",
            "Epoch 5, Train Accuracy:  50.00%\n",
            "Epoch 6, Train Accuracy:  54.62%\n",
            "Epoch 7, Train Accuracy:  51.54%\n",
            "Epoch 8, Train Accuracy:  49.23%\n",
            "Epoch 9, Train Accuracy:  56.15%\n",
            "Epoch 10, Train Accuracy:  46.15%\n",
            "Epoch 11, Train Accuracy:  45.38%\n",
            "Epoch 12, Train Accuracy:  53.85%\n",
            "Epoch 13, Train Accuracy:  46.92%\n",
            "Early stopping\n",
            "Fold: 5\n",
            "Fold: 5\n",
            "Epoch 1, Train Accuracy:  56.15%\n",
            "Epoch 2, Train Accuracy:  44.62%\n",
            "Epoch 3, Train Accuracy:  47.69%\n",
            "Epoch 4, Train Accuracy:  54.62%\n",
            "Epoch 5, Train Accuracy:  48.46%\n",
            "Early stopping\n",
            "Fold: 6\n",
            "Fold: 6\n",
            "Epoch 1, Train Accuracy:  52.31%\n",
            "Epoch 2, Train Accuracy:  54.62%\n",
            "Epoch 3, Train Accuracy:  55.38%\n",
            "Epoch 4, Train Accuracy:  50.00%\n",
            "Epoch 5, Train Accuracy:  48.46%\n",
            "Epoch 6, Train Accuracy:  48.46%\n",
            "Epoch 7, Train Accuracy:  50.77%\n",
            "Early stopping\n",
            "Fold: 7\n",
            "Fold: 7\n",
            "Epoch 1, Train Accuracy:  54.62%\n",
            "Epoch 2, Train Accuracy:  50.77%\n",
            "Epoch 3, Train Accuracy:  49.23%\n",
            "Epoch 4, Train Accuracy:  56.15%\n",
            "Epoch 5, Train Accuracy:  52.31%\n",
            "Epoch 6, Train Accuracy:  52.31%\n",
            "Epoch 7, Train Accuracy:  47.69%\n",
            "Epoch 8, Train Accuracy:  46.92%\n",
            "Early stopping\n",
            "Fold: 8\n",
            "Fold: 8\n",
            "Epoch 1, Train Accuracy:  49.23%\n",
            "Epoch 2, Train Accuracy:  50.77%\n",
            "Epoch 3, Train Accuracy:  53.85%\n",
            "Epoch 4, Train Accuracy:  50.77%\n",
            "Epoch 5, Train Accuracy:  54.62%\n",
            "Epoch 6, Train Accuracy:  42.31%\n",
            "Epoch 7, Train Accuracy:  48.46%\n",
            "Epoch 8, Train Accuracy:  54.62%\n",
            "Epoch 9, Train Accuracy:  46.92%\n",
            "Early stopping\n",
            "Fold: 9\n",
            "Fold: 9\n",
            "Epoch 1, Train Accuracy:  49.23%\n",
            "Epoch 2, Train Accuracy:  52.31%\n",
            "Epoch 3, Train Accuracy:  50.77%\n",
            "Epoch 4, Train Accuracy:  50.77%\n",
            "Epoch 5, Train Accuracy:  49.23%\n",
            "Epoch 6, Train Accuracy:  44.62%\n",
            "Early stopping\n",
            "Fold: 10\n",
            "Fold: 10\n",
            "Epoch 1, Train Accuracy:  39.23%\n",
            "Epoch 2, Train Accuracy:  60.77%\n",
            "Epoch 3, Train Accuracy:  46.15%\n",
            "Epoch 4, Train Accuracy:  46.92%\n",
            "Epoch 5, Train Accuracy:  51.54%\n",
            "Epoch 6, Train Accuracy:  52.31%\n",
            "Early stopping\n",
            "Final Train Accuracy =  55.62%\n",
            "Accuracy =  52.86%\n",
            "Average Recall =  46.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from numpy.core.multiarray import asarray\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import f1_score\n",
        "!pip install scikit-optimize\n",
        "#ベイズ最適化\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch.nn.init as init\n",
        "import random\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import torch.optim as optim\n",
        "#正解率などを計算\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "#!pip install optuna[visualization]\n",
        "#import torchmetrics\n",
        "#best drop 0.49546994410761214 0.5/0.086\n",
        "#best lr 0.008623697312249823\n",
        "#91.78% accuracy, 92.84% sensitivity(recall), and 90.73% specificity.\n",
        "\n",
        "#drop0.5814981354327653 lr 0.0003952902573873088\n",
        "\n",
        "#ハイパーパラメータ\n",
        "device = torch.device(\"cuda\")\n",
        "num_epochs = 20\n",
        "num_classes = 2\n",
        "batch_size = 13\n",
        "learning_rate = 0.001\n",
        "\n",
        "def time_shift(data, shift_max = 1000):\n",
        "  shift = random.randint(-shift_max, shift_max)\n",
        "  return np.roll(data, shift, axis = 1)\n",
        "\n",
        "def add_noise(data, noise_level=0.01):\n",
        "  noise = np.random.normal(0, noise_level, data.shape)\n",
        "  return data + noise\n",
        "\n",
        "def apply_scaling(data, scale_min=0.8, scale_max=1.2):\n",
        "    scales = np.random.uniform(scale_min, scale_max, data.shape[0])\n",
        "    scaled_data = data * scales[:, np.newaxis]\n",
        "    return scaled_data\n",
        "\n",
        "class CustomDatasetFromCSV(Dataset):\n",
        "  def __init__(self, csv_path, a, b, c, d, length, fold):\n",
        "    self.fold = fold\n",
        "    self.transforms = transforms\n",
        "    self.to_tensor = transforms.ToTensor()\n",
        "    self.data_info = pd.read_csv(csv_path)\n",
        "    if c == 0:\n",
        "      self.image_arr = np.asarray(self.data_info.iloc[a:b, 0])\n",
        "      self.label_arr = np.asarray(self.data_info.iloc[a:b, 1])\n",
        "    else:\n",
        "      self.image_arr = np.concatenate((np.asarray(self.data_info.iloc[a:b, 0]), np.asarray(self.data_info.iloc[c:d, 0])))\n",
        "      self.label_arr = np.concatenate((np.asarray(self.data_info.iloc[a:b, 1]), np.asarray(self.data_info.iloc[c:d, 1])))\n",
        "\n",
        "    #print(f\"Image Array for fold {self.fold}: \", self.image_arr)\n",
        "    print(f\"Fold: {self.fold}\")\n",
        "\n",
        "    self.image_len = length\n",
        "    #print(\"index\", self.image_len)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    single_image_name = self.image_arr[index]\n",
        "\n",
        "    # ここでファイル名を出力（デバッグ用）\n",
        "    #print(f\"Reading data from {single_image_name}.txt\")\n",
        "\n",
        "    lines_img = loadtxt(single_image_name + \".txt\", delimiter=\",\", unpack=False)\n",
        "\n",
        "    # Debugging print statements\n",
        "    #print(\"Shape of lines_img before reshape:\", lines_img.shape)\n",
        "\n",
        "    #データ拡張の適用\n",
        "    #lines_img = time_shift(lines_img) #時間的シフト\n",
        "    #lines_img = add_noise(lines_img) #ノイズ追加\n",
        "    #lines_img = apply_scaling(lines_img)\n",
        "\n",
        "    pad_size = 10240 - lines_img.shape[1]\n",
        "    padded_lines_img = np.pad(lines_img, [(0,0), (0, pad_size)], \"constant\")\n",
        "    padded_lines_img = np.reshape(padded_lines_img, (1, 64, 10240))\n",
        "    img_as_tensor = torch.FloatTensor(padded_lines_img)\n",
        "\n",
        "    #lines_img = np.reshape(lines_img, (1,64,10240))\n",
        "\n",
        "    single_image_label = self.label_arr[index]\n",
        "    single_image_label = torch.LongTensor([single_image_label])\n",
        "    #single_image_label = torch.tensor([single_image_label], dtype = torch.long)\n",
        "    return (img_as_tensor, single_image_label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.image_len\n",
        "\n",
        "\"\"\"\n",
        "class CustomDatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, a, b, c, d, length, fold):\n",
        "        self.fold = fold\n",
        "        self.data_info = pd.read_csv(csv_path)\n",
        "        self.preloaded_data = []\n",
        "        self.label_arr = []\n",
        "\n",
        "        # データのパスを取得\n",
        "        if c == 0:\n",
        "            image_paths = np.asarray(self.data_info.iloc[a:b, 0])\n",
        "            self.label_arr = np.asarray(self.data_info.iloc[a:b, 1])\n",
        "        else:\n",
        "            image_paths = np.concatenate((np.asarray(self.data_info.iloc[a:b, 0]), np.asarray(self.data_info.iloc[c:d, 0])))\n",
        "            self.label_arr = np.concatenate((np.asarray(self.data_info.iloc[a:b, 1]), np.asarray(self.data_info.iloc[c:d, 1])))\n",
        "\n",
        "        # すべてのデータを一度にメモリにロード\n",
        "        for img_path in image_paths:\n",
        "            lines_img = loadtxt(img_path + \".txt\", delimiter=\",\", unpack=False)\n",
        "            lines_img = time_shift(lines_img)\n",
        "            lines_img = add_noise(lines_img)\n",
        "            pad_size = 10240 - lines_img.shape[1]\n",
        "            padded_lines_img = np.pad(lines_img, [(0,0), (0, pad_size)], \"constant\")\n",
        "            padded_lines_img = np.reshape(padded_lines_img, (1, 64, 10240))\n",
        "            self.preloaded_data.append(padded_lines_img)\n",
        "\n",
        "        self.preloaded_data = np.array(self.preloaded_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # メモリにロードされたデータを直接使用\n",
        "        img_as_tensor = torch.FloatTensor(self.preloaded_data[index])\n",
        "        single_image_label = torch.tensor([self.label_arr[index]], dtype=torch.long)\n",
        "        return (img_as_tensor, single_image_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.preloaded_data)\n",
        "\"\"\"\n",
        "\n",
        "#CNNモデルの定義\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, (1, 11)),\n",
        "        nn.Conv2d(32, 32, (64,1)),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(1,2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(32, 20, (1, 10)),\n",
        "        nn.BatchNorm2d(20),\n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(1, 4)\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Conv2d(20, 20, (1, 10)),\n",
        "        nn.BatchNorm2d(20),\n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(1,4)\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Conv2d(20, 20, (1, 11)),\n",
        "        nn.BatchNorm2d(20),\n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(1,3)\n",
        "    )\n",
        "    self.drop_out = nn.Dropout(0.5)\n",
        "    self.fc1 = nn.Linear(2060, 100)\n",
        "    self.fc2 = nn.Linear(100, 2)\n",
        "    #self.fc3 = nn.Linear(50, 2)\n",
        "    self._initialize_weights()\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        init.constant_(m.bias, 0)\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    #print(f\"Before entering fc1, shape of out: {out.shape}\")\n",
        "    out = self.drop_out(out)\n",
        "    #out = F.relu(self.fc1(out))\n",
        "    out = self.fc1(out)\n",
        "    out = self.fc2(out)\n",
        "    #out = F.relu(self.fc2(out))\n",
        "    #out = self.drop_out(out)\n",
        "    #out = self.fc3(out)\n",
        "    return out\n",
        "\n",
        "\"\"\"\n",
        "#optuna用のデータロード\n",
        "optuna_train_dataset = CustomDatasetFromCSV(\"/content/A.csv\", 10, 50, 0, 0, 30, 1)\n",
        "train_loader = torch.utils.data.DataLoader(optuna_train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
        "\n",
        "optuna_val_dataset = CustomDatasetFromCSV(\"/content/A.csv\", 60, 70, 0, 0, 10, 1)\n",
        "val_loader = torch.utils.data.DataLoader(optuna_val_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
        "\n",
        "#optunaによるハイパーパラメータ最適化\n",
        "def objective(trial):\n",
        "  dropout_rate = trial.suggest_float(\"dropout_rate\", 0.3, 0.7)\n",
        "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-2)\n",
        "\n",
        "  model = ConvNet(num_classes, dropout_rate).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay=1e-5)\n",
        "\n",
        "  #モデルの訓練+val\n",
        "  num_epochs=10\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      #print(\"訓練用の正解ラベル = \", labels)\n",
        "      labels = labels.view(-1)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    #検証ループ\n",
        "    val_accuracy = calculate_val_accuracy(model, val_loader)\n",
        "    print(f'Epoch {epoch}: Validation Accuracy = {val_accuracy}%')\n",
        "\n",
        "    #Pruner\n",
        "    try:\n",
        "      trial.report(val_accuracy, epoch)\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "    except optuna.exceptions.TrialPruned:\n",
        "      print(f\"Trial {trial.number} early stopped at epoch {epoch}.\")\n",
        "      raise\n",
        "  return val_accuracy\n",
        "\n",
        "#ハイパーパラメータ最適化のval精度の計算\n",
        "def calculate_val_accuracy(model, val_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            labels = labels.squeeze()\n",
        "            print(f\"Predicted: {predicted}\")\n",
        "            print(f\"Labels: {labels}\")\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print(f\"Total: {total}, Correct: {correct}\")\n",
        "    val_accuracy = 100 * correct / total\n",
        "    return val_accuracy\n",
        "\n",
        "#Pruner\n",
        "pruner = optuna.pruners.MedianPruner(\n",
        "    n_startup_trials=5,  # 最初の2トライアルはプルーニングしない\n",
        "    n_warmup_steps=2,    # 最初からプルーニングのチェックを開始\n",
        "    interval_steps=1     # 1エポックごとにプルーニングのチェックを行う\n",
        ")\n",
        "\n",
        "#optuna実行\n",
        "study = optuna.create_study(direction=\"maximize\", pruner = pruner)\n",
        "study.optimize(objective, n_trials=20)\n",
        "#進捗の可視化\n",
        "#optuna.visualization.plot_optimization_history(study)\n",
        "#最高のハイパーパラメータを設定\n",
        "best_dropout = study.best_params[\"dropout_rate\"]\n",
        "best_lr = study.best_params[\"lr\"]\n",
        "\"\"\"\n",
        "\n",
        "#10-fold交差検証によるモデルの訓練とテスト\n",
        "ns = 14\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "# 各指標の合計値を初期化\n",
        "total_accuracy = 0\n",
        "total_precision = 0\n",
        "total_recall = 0\n",
        "total_f1 = 0\n",
        "total_specificity = 0\n",
        "for fold in range(1, 11):\n",
        "  if fold == 1:\n",
        "    a1_train = 14\n",
        "    a2_train = 144\n",
        "    a1_test = 0\n",
        "    a2_test = 14\n",
        "    custom_mnist_from_csv_train = CustomDatasetFromCSV(\"/content/A.csv\", a1_train, a2_train, 0, 0, 130, fold)\n",
        "    custom_mnist_from_csv_test = CustomDatasetFromCSV(\"/content/A.csv\", a1_test, a2_test, 0, 0, 14, fold)\n",
        "  elif fold == 10:\n",
        "    a1_train = 0\n",
        "    a2_train = 130\n",
        "    a1_test = 130\n",
        "    a2_test = 144\n",
        "    custom_mnist_from_csv_train = CustomDatasetFromCSV(\"/content/A.csv\", a1_train, a2_train, 0, 0, 130, fold)\n",
        "    custom_mnist_from_csv_test = CustomDatasetFromCSV(\"/content/A.csv\", a1_test, a2_test, 0, 0, 14, fold)\n",
        "  else:\n",
        "    a1_train = 0\n",
        "    a2_train = (fold-2)*ns\n",
        "    b1_train = (fold-1)*ns\n",
        "    b2_train = 144\n",
        "    a1_test = a2_train\n",
        "    a2_test = b1_train\n",
        "    custom_mnist_from_csv_train = CustomDatasetFromCSV(\"/content/A.csv\", a1_train, a2_train, b1_train, b2_train, 130, fold)\n",
        "    custom_mnist_from_csv_test = CustomDatasetFromCSV(\"/content/A.csv\", a1_test, a2_test, 0, 0, 14, fold)\n",
        "\n",
        "  #訓練データをロードする(13から10へ)\n",
        "  dataset = torch.utils.data.DataLoader(dataset = custom_mnist_from_csv_train, batch_size=10, shuffle=True, num_workers=8)\n",
        "  #モデルを設定\n",
        "  model = ConvNet(num_classes).to(device)\n",
        "  #model = ConvNet(num_classes, best_dropout).to(device) #optuna\n",
        "  #重み初期化\n",
        "  #model._initialize_weights()\n",
        "  #モデルをCudaに移す\n",
        "  model = model.cuda()\n",
        "\n",
        "  #損失関数設定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #optimizer設定\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
        "  #optimizer = torch.optim.Adam(model.parameters(), lr = best_lr, weight_decay=1e-5)\n",
        "\n",
        "  total_step = len(dataset)\n",
        "\n",
        "  sumtrain = 0\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  best_accuracy = 0.0\n",
        "  patience = 4\n",
        "  counter = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    #print(\"train_accuracy = \", sumtrain)\n",
        "    #print(\"loss = \", loss)\n",
        "\n",
        "    total_train_data = 0\n",
        "    sumtrain = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataset):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      total_train_data += len(labels)\n",
        "\n",
        "      outputs = model(images)\n",
        "      labels = labels.view(-1)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      outputs = torch.cuda.FloatTensor(outputs)\n",
        "      #outputs = torch.tensor(outputs, dtype=torch.float, device='cuda')\n",
        "\n",
        "      correct = 0\n",
        "      for j in range(len(outputs)):\n",
        "        if (outputs[j, 0] < outputs[j, 1]) and torch.eq(labels[j], 1):\n",
        "          correct += 1\n",
        "        elif (outputs[j, 0] > outputs[j, 1]) and torch.eq(labels[j], 0):\n",
        "          correct += 1\n",
        "\n",
        "      sumtrain += correct\n",
        "    #エポックごとの訓練精度\n",
        "    train_accuracy = (sumtrain / total_train_data) * 100\n",
        "    print(f\"Epoch {epoch+1}, Train Accuracy: {train_accuracy: .2f}%\")\n",
        "\n",
        "    #Early Stopping\n",
        "    if train_accuracy > best_accuracy:\n",
        "      best_accuracy = train_accuracy\n",
        "      counter = 0\n",
        "    else:\n",
        "      counter += 1\n",
        "    if counter >= patience:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "  #各foldごとの訓練精度\n",
        "  train_accuracies.append(best_accuracy)\n",
        "\n",
        "  #テストデータロード。num_workersで並列処理をして高速化\n",
        "  dataset = torch.utils.data.DataLoader(dataset=custom_mnist_from_csv_test, batch_size=7, shuffle=True, num_workers=8)\n",
        "#9から７へ\n",
        "  #f1 score用\n",
        "  P_labels = []\n",
        "  T_labels = []\n",
        "  #テスト用のバッチ\n",
        "  for i, (images, labels) in enumerate(dataset):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct += (predicted==labels).sum().item()\n",
        "    P_labels.extend(predicted.cpu().numpy())\n",
        "    T_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  # 追加する指標の計算\n",
        "  total_accuracy += accuracy_score(T_labels, P_labels) * 100\n",
        "  total_precision += precision_score(T_labels, P_labels) * 100\n",
        "  total_recall += recall_score(T_labels, P_labels) * 100\n",
        "  total_f1 += f1_score(T_labels, P_labels)\n",
        "  tn, fp, fn, tp = confusion_matrix(T_labels, P_labels).ravel()\n",
        "  total_specificity += tn / (tn + fp) * 100\n",
        "\n",
        "\n",
        "# 最終的な平均値を計算\n",
        "average_train_accuracy = sum(train_accuracies) / len(train_accuracies)\n",
        "average_accuracy = total_accuracy / 10\n",
        "average_precision = total_precision / 10\n",
        "average_recall = total_recall / 10\n",
        "average_f1 = total_f1 / 10\n",
        "average_specificity = total_specificity / 10\n",
        "\n",
        "# 結果の出力\n",
        "print(f\"Final Train Accuracy = {average_train_accuracy: .2f}%\")\n",
        "print(f\"Accuracy = {average_accuracy: .2f}%\")\n",
        "print(f\"Average Recall = {average_recall: .2f}%\")\n",
        "#print(f\"Average Precision: = {average_precision: .2f}%\")\n",
        "#print(\"Average F1 Score:\", average_f1)\n",
        "#print(\"Average Specificity:\", average_specificity)\n",
        "\n",
        "#出力\n",
        "#print(\"Length of train_accuracies\", len(train_accuracies))\n",
        "#print(f\"Final Train Accuracy = {average_train_accuracy: .2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AY6pbDB2-P69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c9d728-0875-4e25-aac0-d37e73c531f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Train Accuracy =  54.54%\n",
            "Accuracy =  42.86%\n",
            "Average Recall =  43.05%\n",
            "Average Specificity: =  43.23%\n",
            "Average Precision: =  38.73%\n",
            "Average F1 Score: 0.36925293305728085\n"
          ]
        }
      ],
      "source": [
        "print(f\"Final Train Accuracy = {average_train_accuracy: .2f}%\")\n",
        "print(f\"Accuracy = {average_accuracy: .2f}%\")\n",
        "print(f\"Average Recall = {average_recall: .2f}%\")\n",
        "print(f\"Average Specificity: = {average_specificity: .2f}%\")\n",
        "\n",
        "print(f\"Average Precision: = {average_precision: .2f}%\")\n",
        "print(\"Average F1 Score:\", average_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krs3oZB77cC6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8qXy6bh7cFp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1B-ORbyBXrh7fEMzsv14rG9h0Y5WiSITG",
      "authorship_tag": "ABX9TyPHGcK9ZGyQEptbVUxxsDvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}